{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PART 1. Using NLTK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "text = \"For those of you who have read the first edition of this book, some of the familiar case studies \\\n",
    "        will reappear in this edition\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: \n",
      "======\n",
      " ['For', 'those', 'of', 'you', 'who', 'have', 'read', 'the', 'first', 'edition', 'of', 'this', 'book', ',', 'some', 'of', 'the', 'familiar', 'case', 'studies', 'will', 'reappear', 'in', 'this', 'edition']\n"
     ]
    }
   ],
   "source": [
    "# 1. Split an input text into tokens including words and marks such as comma, punctuation, etc...\n",
    "tokens = nltk.word_tokenize(text)\n",
    "print(\"Output: \\n======\\n\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stopwords:  179\n",
      "Output: \n",
      "======\n",
      " ['For', 'read', 'first', 'edition', 'book', ',', 'familiar', 'case', 'studies', 'reappear', 'edition']\n"
     ]
    }
   ],
   "source": [
    "# 2. Get stopword list, then removing stopwords\n",
    "stopwords = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "print(\"Number of stopwords: \", len(stopwords))\n",
    "\n",
    "tokens = [token for token in tokens if token not in stopwords]\n",
    "print(\"Output: \\n======\\n\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: \n",
      "======\n",
      " [('For', 'IN'), ('read', 'VBN'), ('first', 'JJ'), ('edition', 'NN'), ('book', 'NN'), (',', ','), ('familiar', 'JJ'), ('case', 'NN'), ('studies', 'NNS'), ('reappear', 'VBP'), ('edition', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# 3. Tag part-of-speech (POS) for each token in the input text\n",
    "tagged_tokens = nltk.pos_tag(tokens)\n",
    "print(\"Output: \\n======\\n\", tagged_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: \n",
      "======\n",
      " (S\n",
      "  For/IN\n",
      "  read/VBN\n",
      "  first/JJ\n",
      "  edition/NN\n",
      "  book/NN\n",
      "  ,/,\n",
      "  familiar/JJ\n",
      "  case/NN\n",
      "  studies/NNS\n",
      "  reappear/VBP\n",
      "  edition/NN)\n"
     ]
    }
   ],
   "source": [
    "# 4. Identify name entities\n",
    "# The input for ne_chunk() is the list of tokens that were tagged the part-of-speech\n",
    "entities = nltk.chunk.ne_chunk(tagged_tokens)\n",
    "print(\"Output: \\n======\\n\", entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PART 2. Using Gensim to convert docs to TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "documents = [\n",
    "    ['architecture', 'layers', 'request', 'flows'],\n",
    "    ['event', 'processor', 'components'],\n",
    "    ['researchers', 'struggled', 'MLPs', 'backpropagation', 'autodiff'],\n",
    "    ['determine', 'variance', 'estimators'],\n",
    "    ['summarize', 'hypothesis', 'tests', 'mean'],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'architecture'), (1, 'flows'), (2, 'layers'), (3, 'request'), (4, 'components'), (5, 'event'), (6, 'processor'), (7, 'MLPs'), (8, 'autodiff'), (9, 'backpropagation'), (10, 'researchers'), (11, 'struggled'), (12, 'determine'), (13, 'estimators'), (14, 'variance'), (15, 'hypothesis'), (16, 'mean'), (17, 'summarize'), (18, 'tests')]\n"
     ]
    }
   ],
   "source": [
    "# 1. Create the dictionary from the input document set\n",
    "# Dictionary is a list of tuples, in which each tuple is a pair of word_id and word\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "print(list(dictionary.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document 0 has 4 words:  [(0, 1), (1, 1), (2, 1), (3, 1)]\n",
      "document 1 has 3 words:  [(4, 1), (5, 1), (6, 1)]\n",
      "document 2 has 5 words:  [(7, 1), (8, 1), (9, 1), (10, 1), (11, 1)]\n",
      "document 3 has 3 words:  [(12, 1), (13, 1), (14, 1)]\n",
      "document 4 has 4 words:  [(15, 1), (16, 1), (17, 1), (18, 1)]\n"
     ]
    }
   ],
   "source": [
    "# 2. Represent each document in bag-of-word\n",
    "# in each document, a word is represented by a pair of the word_id and the frequency of this word in the document\n",
    "corpus = [dictionary.doc2bow(document) for document in documents]\n",
    "for num, document in enumerate(corpus):\n",
    "    print(\"document %d has %d words: \" %(num, len(document)), document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document 0 has 4 words:  [(0, 0.5), (1, 0.5), (2, 0.5), (3, 0.5)]\n",
      "document 1 has 3 words:  [(4, 0.5773502691896258), (5, 0.5773502691896258), (6, 0.5773502691896258)]\n",
      "document 2 has 5 words:  [(7, 0.447213595499958), (8, 0.447213595499958), (9, 0.447213595499958), (10, 0.447213595499958), (11, 0.447213595499958)]\n",
      "document 3 has 3 words:  [(12, 0.5773502691896258), (13, 0.5773502691896258), (14, 0.5773502691896258)]\n",
      "document 4 has 4 words:  [(15, 0.5), (16, 0.5), (17, 0.5), (18, 0.5)]\n"
     ]
    }
   ],
   "source": [
    "# 3. convert bag-of-word to numeric vector using tf-idf\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "tfidf_corpus = tfidf[corpus]\n",
    "for num, document in enumerate(tfidf_corpus):\n",
    "    print(\"document %d has %d words: \" %(num, len(document)), document)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
